{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71dfc09-55e2-4d52-9d95-fd3cc90c8cd8",
   "metadata": {},
   "source": [
    "# Optimising the GAN to produce flow duration and flow size using Optuna\n",
    "\n",
    "Following three examples:\n",
    "\n",
    "1. https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837\n",
    "This example is a simple example to follow.\n",
    "\n",
    "2. https://github.com/optuna/optuna-examples/blob/main/multi_objective/pytorch_simple.py\n",
    "This example is more complicated but demonstrates how to have multi-objective optimisation, which is key in optimising the GAN, where we need to minimise loss for both neural networks.\n",
    "\n",
    "3. https://gitlab.com/hpo-uq/applications/gan4hep/-/blob/main/gan4hep/train_gan_2angles.py?ref_type=heads\n",
    "This example comes from the HYPPO paper, 'gan4hep' module.\n",
    "\n",
    "**NOTE (What is different in version 3 - 28/01/2024):** \n",
    "\n",
    "(27/01/2024)\n",
    "I am going to try an add my own function that accesses the study within the train function. This will be triggered when a trial is half way completed, and when there are at least five trials completed. It will take the median value of the generator losses and the median value of the discriminator losses from previous trials. If the current generator loss or discriminator loss is greater than the median value then it will return a boolean True to say that this trial should be pruned. It will also reutrn True if either one of the losses equal 0. This means that the network is no longer learning. \n",
    "\n",
    "The pruner function waits until there are at least 10 complete trials to start pruning, and after that points waits until the trial reaches halfway through training to trigger the pruner. \n",
    "\n",
    "The checkpoint feature seems to be working as expected. Just needed to change the conditions used.\n",
    "\n",
    "The weights and biases of the neural networks are initialised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e0c4355-74fa-4577-9cd9-0f198d3df65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import functools\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f532ca8",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca766cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_LENGTH = 3063456\n",
    "MIN_TRIALS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ec6f6-5ef3-4067-b87d-66b0276d0d32",
   "metadata": {},
   "source": [
    "## Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f04d43b5-e2d7-46d5-8049-7ca51f189848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is 2D, first hidden layer is composed of 256 neurons with ReLU activation\n",
    "            nn.Linear(2, 128), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Have to use dropout to avoid overfitting\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # second and third layers are composed to 128 and 64 neurons, respectively\n",
    "            nn.Linear(128, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # output is composed of a single neuron with sigmoidal activation to represent a probability\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee49249-76f2-4f9b-8803-f6d4c3630004",
   "metadata": {},
   "source": [
    "## Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7990e8e5-a832-4ee7-bbaf-0f45a27adb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21f1e6",
   "metadata": {},
   "source": [
    "## Weights and biases function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b37cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_biases_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        nn.init.kaiming_normal_(model.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92660876-6c93-402c-a7a2-c0ace1f03703",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3ce02bc-0c1f-4c4b-ac37-1acfc41c9ffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_LENGTH = 1024\n",
    "\n",
    "def train_data_length(data, length):\n",
    "    return data[:length]\n",
    "    \n",
    "def load_data():\n",
    "    data = torch.load(\"data.pt\")\n",
    "    data = data.to(torch.float32)\n",
    "    train_data = train_data_length(data,TRAINING_DATA_LENGTH)\n",
    "    return train_data\n",
    "    \n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa92aaf",
   "metadata": {},
   "source": [
    "## Custom Pruner function \n",
    "Because the in-built Optuna pruner Class cannot be used for multi-objective optimisation, a custom pruner function was written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trial(study, current_g_loss, current_d_loss):\n",
    "    \n",
    "    should_prune = False\n",
    "    \n",
    "    if current_g_loss == 0.0 or current_d_loss == 0.0: # one of the networks are no longer learning\n",
    "        should_prune = True\n",
    "    \n",
    "    else: \n",
    "        historic_g_loss = []\n",
    "        historic_d_loss = []\n",
    "    \n",
    "        for trial in study.trials:\n",
    "            # Only considering/calculating trials that have been completed\n",
    "            if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "                historic_g_loss.append(trial.values[0]) # accessing and appending the historic value for generator loss\n",
    "                historic_d_loss.append(trial.values[1]) # doing the same for the discriminator loss\n",
    "        \n",
    "                # Convert to numpy arrays\n",
    "                array_g_loss = np.array(historic_g_loss)\n",
    "                array_d_loss = np.array(historic_d_loss)\n",
    "        \n",
    "                # Calculate the mean losses\n",
    "                mean_g_loss = np.mean(array_g_loss)\n",
    "                mean_d_loss = np.mean(array_d_loss)\n",
    "\n",
    "        should_prune = current_g_loss > mean_g_loss or current_d_loss > mean_d_loss\n",
    "        \n",
    "    return should_prune\n",
    "\n",
    "    \n",
    "def prune(study, min_trials, min_epochs, current_epoch, current_g_loss, current_d_loss):\n",
    "    should_prune = False\n",
    "    completed_trials = 0\n",
    "    \n",
    "    # checking how many trials are complete to compare to the current trial\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            completed_trials += 1\n",
    "    \n",
    "    # if there are sufficient trials to compare, it will compare\n",
    "    if completed_trials >= min_trials:\n",
    "        if current_epoch == min_epochs:\n",
    "            should_prune = evaluate_trial(study, current_g_loss, current_d_loss)\n",
    "            if should_prune:\n",
    "                print(\"Trial has been pruned\")\n",
    "                raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924cc5d5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_length(data, length):\n",
    "    return data[:length]\n",
    "    \n",
    "def load_data():\n",
    "    data = torch.load(\"data.pt\")\n",
    "    data = data.to(torch.float32)\n",
    "    train_data = train_data_length(data,TRAINING_DATA_LENGTH)\n",
    "    return train_data\n",
    "    \n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a89e0403-1680-4896-bc3b-2a6c201b2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_optimise(params, generator, discriminator, study):\n",
    "\n",
    "    # loading and moving the training data and models to the GPU if it is available\n",
    "    train_data = load_data()\n",
    "    train_data = train_data.to(DEVICE)\n",
    "    train_labels = torch.zeros(size=(TRAINING_DATA_LENGTH, 1))\n",
    "    train_labels = train_labels.to(DEVICE)\n",
    "    train_set = [(train_data[i], train_labels[i]) for i in range(TRAINING_DATA_LENGTH)]\n",
    "    train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"], shuffle=True, drop_last = True)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    discriminator_optimiser = optim.Adam(discriminator.parameters(), lr = params[\"learning_rate\"])\n",
    "    generator_optimiser = optim.Adam(generator.parameters(), lr = params[\"learning_rate\"])\n",
    "\n",
    "    \n",
    "    discriminator = discriminator.to(DEVICE)\n",
    "    generator = generator.to(DEVICE)\n",
    "    loss_function = loss_function.to(DEVICE)\n",
    "    \n",
    "    warmup_epochs = params[\"epochs\"] // 2\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        \n",
    "        for n, (real_samples, _) in enumerate(tqdm(train_loader)):\n",
    "            \n",
    "            # DATA FOR DISCRIMINATOR\n",
    "            torch.manual_seed(RANDOM_SEED)\n",
    "            real_samples_labels = torch.ones((params[\"batch_size\"], 1), device = DEVICE)\n",
    "            \n",
    "            latent_space_samples = torch.randn((params[\"batch_size\"], 2), device = DEVICE)\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            generated_samples_labels = torch.zeros((params[\"batch_size\"], 1), device = DEVICE)\n",
    "            \n",
    "            all_samples = torch.cat((real_samples, generated_samples))\n",
    "            all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            discriminator.zero_grad()\n",
    "            output_discriminator = discriminator(all_samples)\n",
    "            discriminator_loss = loss_function(output_discriminator, all_samples_labels)\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimiser.step()\n",
    "\n",
    "\n",
    "            # DATA FOR GENERATOR\n",
    "            torch.manual_seed(RANDOM_SEED)\n",
    "            latent_space_samples = torch.randn((params[\"batch_size\"], 2), device = DEVICE)\n",
    "\n",
    "            # TRAINING GENERATOR\n",
    "            generator.zero_grad()\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            output_discriminator_generated = discriminator(generated_samples)\n",
    "            generator_loss = loss_function(output_discriminator_generated, real_samples_labels)\n",
    "            generator_loss.backward()\n",
    "            generator_optimiser.step()\n",
    "\n",
    "            if epoch % 10 == 0 and n == params[\"batch_size\"]:\n",
    "                print(f\"Epoch: {epoch} | G. Loss: {generator_loss} | D. Loss: {discriminator_loss}\")\n",
    "\n",
    "        # Pruning\n",
    "        prune(study = study, min_trials = MIN_TRIALS, min_epochs = warmup_epochs, current_epoch = epoch, \n",
    "              current_g_loss = generator_loss, current_d_loss = discriminator_loss)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    run_time = round(end_time - start_time, 2)\n",
    "    print(f\"Trial Complete!\\nRun time for this trial was {run_time} seconds.\\n\")\n",
    "\n",
    "    return generator_loss, discriminator_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235a5db-4dcf-4d0b-a145-bfb2b37701aa",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "The aim of this function is to define a set of hyperparameter values, build the model, train the model, and evaluate the loss of both the generator and discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4efa56d8-2e74-4375-aaeb-c6c1ebf8c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, additional_arg):\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", low=1e-5, high=1e-1, log = True),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", low=1000, high=10000, step=1000),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", low=5, high=50, step=5),\n",
    "        \n",
    "    }\n",
    "\n",
    "    discriminator = Discriminator()\n",
    "    discriminator.apply(weights_biases_init)\n",
    "    \n",
    "    generator = Generator()\n",
    "    generator.apply(weights_biases_init)\n",
    "\n",
    "    g_loss, d_loss = train_and_optimise(params, generator, discriminator, additional_arg)\n",
    "\n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df75fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(t: optuna.trial.FrozenTrial):\n",
    "    return t.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b441bf4-f807-4d7f-8186-3169bd427e8c",
   "metadata": {},
   "source": [
    "## Display all trials and best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20a71587-7270-4e35-9e7a-abcf7dd61751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_trials(study):\n",
    "    df = study.trials_dataframe()\n",
    "    df = pd.DataFrame(df, columns = ['number', 'values_0', 'values_1', 'params_batch_size', 'params_epochs',\n",
    "                                          'params_learning_rate', 'state'])\n",
    "    df= df.rename(columns = {\"number\":\"Trial #\", \"values_0\":\"G Loss\", \"values_1\":\"D Loss\", \"params_batch_size\":\"Batch Size\", \n",
    "                     \"params_epochs\":\"Epochs\", \"params_learning_rate\":\"Learning Rate\", \"state\":\"State\"})\n",
    "    df[\"Trial #\"] += 1 # Adjust the trial numbers\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "def display_best_trial(study):\n",
    "    best_trial = study.best_trials\n",
    "    best_trial_number = best_trial[0].number\n",
    "    best_trial_params = best_trial[0].params\n",
    "    best_trial_values = best_trial[0].values\n",
    "    print(\"~Best trial~\")\n",
    "    print(f\"Trial #: {best_trial_number}\")\n",
    "    print(f\"G Loss: {best_trial_values[0]}\\nD Loss: {best_trial_values[1]}\")\n",
    "    print(f\"Batch Size: {best_trial_params['batch_size']}\\nEpochs: {best_trial_params['epochs']}\\nLearning Rate: {best_trial_params['learning_rate']}\")\n",
    "\n",
    "def save_trials(study):\n",
    "    df = study.trials_dataframe()\n",
    "    now = str(datetime.now())\n",
    "    date, time = now.split(\" \")\n",
    "    time = time.replace(\":\",\".\")\n",
    "    df.to_csv(f\"Optimisation Trials/gan_optimisation_{date}_{time}.csv\", index = False)\n",
    "    print(f\"Trials saved to: gan_optimisation_{date}_{time}.csv\\n\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f0c69c1-f88c-44b4-82be-a259ae4495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(seconds):\n",
    "    seconds = seconds % (24 * 3600)\n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "     \n",
    "    return \"%d hours %02d mins %02d secs\" % (hour, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbae1bc-d829-465e-92ef-0c3f0bf2d421",
   "metadata": {},
   "source": [
    "## Main program\n",
    "The study that is created provides a multi-objective optimisation, so that it can optimise more than one value - generator loss and discriminator loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f85c9b09-dc94-47a0-b1c3-c93af0d2c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-04 18:01:24,992] A new study created in memory with name: GAN-Optimiser\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0abb4ae332486eb1d04391b6cc0487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | G. Loss: 3.39906644821167 | D. Loss: 50.029754638671875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e093d116528457a92212bffb463394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f85140a977e4e9c8c739795a4c00414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58a58be5f1e44bf9d8fd0fd94116e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4b1675e4e2499b850d73d98e0583d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b19f817f1b04f63aae136add62d0452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e23b6e03e3d4c33b9f73e1777a2be7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e577be3d578b49c7bbe1cd9d88cf68b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5039951c13404c0296b3bd8c60e064fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d519d054344f3e9bb3becff93cbd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1b7037b1984457a90bcc5ff8d4820a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | G. Loss: 15.07404899597168 | D. Loss: 50.0000114440918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bba6f3e0d4c49b6aa2d9a4d6bda8c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efc223b21ad4eeca4d0343d69db0abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a44eba06fb45b884e7728155b95606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6571b98b74754fcc8b7301356849c53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce17057e28a4174ad5ffdcf756d0cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e845f71f801648929eaa12278035b8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a8aa85ec514a0d9302fbdb0ec6dd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9899c8cb00d4ee39762aa42984a75d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762a9571ddfc4cf99159a230153a3470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0410cd73870444b99a6a974d564ba6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | G. Loss: 16.876033782958984 | D. Loss: 50.00000762939453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0978d2ad9f487b8c3dcb067a972c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9c1886c2d34faeaeddf36282b97562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fc12ccadd540778a428fc7bfccc9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaf8d6bd915435dad99661e70e0d61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed7e88ef2cb4515998d1b4cc7461013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0e168b82fe4d3785c64a751a43443c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438ab87c97164be6bf5b22dd7abe4ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7b58da6a994cc9960f30f8aea545ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d73182be2ba495ba7a12de050e42d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf4ac3fb99941e283a4a0f02ae0ff8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | G. Loss: 18.943466186523438 | D. Loss: 50.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e59eac57494c3c9b180f7ba05fd02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7224205800439dbe9d8340425edcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda3d98471ef415cb2d92e51e30b9135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfbd5677923495f95df4e151021ba45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd188804893e49f1bc377f899849b1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d41005b191245ec92774cf03589567a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23688eb7bc20455eb78022aed661350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d28f3a94b740aab879c356108b6646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fea9c62d734fe38a1a025c2c1e3cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a2f2713ca04726834d3a4c994becdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | G. Loss: 20.17980194091797 | D. Loss: 50.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898fe5f998be427186602e301bd1c140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef179cf92be475b8eeb16a5023ed907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1c2bcfd102400b84ace953b1a33c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88720ff2c6024c099029483362bdecf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b2e82a9ba64aaca1aa5e301582a480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9da0e609e441afa63c04fd6fda7592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09594e574cbb49f284c0fadb368ff934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f612b6edd947e383419bddfb5d5ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2184f001f8824044a8dbe1e1db3a46c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-04 18:01:46,064] Trial 0 finished with values: [20.944059371948242, 50.0] and parameters: {'learning_rate': 0.0005944115431248001, 'batch_size': 24, 'epochs': 50}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Complete!\n",
      "Run time for this trial was 20.29 seconds.\n",
      "\n",
      "Optimsation complete!\n",
      "Optimisation run time was 0 hours 00 mins 21 secs\n",
      "\n",
      "Number of finished trials:  1\n",
      "Trials saved to: gan_optimisation_2024-01-04_18.01.46.092247.csv\n",
      "\n",
      "   Trial #     G Loss  D Loss  Batch Size  Epochs  Learning Rate     State\n",
      "0        1  20.944059    50.0          24      50       0.000594  COMPLETE\n",
      "\n",
      "\n",
      "~Best trial~\n",
      "Trial #: 0\n",
      "G Loss: 20.944059371948242\n",
      "D Loss: 50.0\n",
      "Batch Size: 24\n",
      "Epochs: 50\n",
      "Learning Rate: 0.0005944115431248001\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    interrupted = False\n",
    "    \n",
    "    # Create the optimisation study\n",
    "    study = optuna.create_study(directions=[\"minimize\", \"minimize\"], study_name = \"GAN-Optimiser\",\n",
    "                                sampler = optuna.samplers.TPESampler(), storage = \"sqlite:///gan_optimisation_pruner_test3_weights_init.db\", load_if_exists=True)\n",
    "\n",
    "    additional_arg = study\n",
    "    wrapped_objective = functools.partial(objective, additional_arg = additional_arg)\n",
    "    \n",
    "    # checking if an optimisation loop was halted and there are incomplete trials\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.FAIL:\n",
    "            interrupted = True\n",
    "            study.enqueue_trial(trial.params)\n",
    "\n",
    "\n",
    "    # Optimise the objective function \n",
    "    if interrupted: # carrying on from previously started optimisation loop\n",
    "        print(\"Continuing previous optimisation loop...\")\n",
    "        study.optimize(wrapped_objective)\n",
    "        \n",
    "        # study.enqueue_trial(study.trials[-1].params)\n",
    "        # study.optimize(objective, n_trials = 1)\n",
    "        \n",
    "    else: # starting a completely new study\n",
    "        print(\"Starting new optimisation loop...\")\n",
    "        start_time = time.time()\n",
    "        study.optimize(wrapped_objective, n_trials = 100) # Number of trials to test with different values\n",
    "        end_time = time.time()\n",
    "\n",
    "    optimisation_run_time = convert_time(round(end_time - start_time, 2))\n",
    "    \n",
    "    print(f\"Optimsation complete!\\nOptimisation run time was {optimisation_run_time}\\n\")\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    save_trials(study)\n",
    "    display_all_trials(study)\n",
    "    display_best_trial(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
