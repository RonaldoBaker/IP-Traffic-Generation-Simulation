{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71dfc09-55e2-4d52-9d95-fd3cc90c8cd8",
   "metadata": {},
   "source": [
    "# Generating flow duration and flow size\n",
    "\n",
    "**NOTE(20/12/23)** So far with the training, the GAN cannot emulate the distribution of training data accurately with a small number of data samples and a smaller batch. Increasing both values seems to improve the accuracy of distribution. Model will need to optimised to find the most appropriate hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e0c4355-74fa-4577-9cd9-0f198d3df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c90ce-048a-4ff3-8d41-85f6186c1d51",
   "metadata": {},
   "source": [
    "## Discriminator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd78ce3-2777-4efb-bf92-0e957a20f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is 2D, first hidden layer is composed of 256 neurons with ReLU activation\n",
    "            nn.Linear(2, 256), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Have to use dropout to avoid overfitting\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # second and third layers are composed to 128 and 64 neurons, respectively\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # output is composed of a single neuron with sigmoidal activation to represent a probability\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547cb7a-56f0-4b1f-aff1-ca168eb01bdc",
   "metadata": {},
   "source": [
    "## Generator class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e1c245-c009-465e-a40d-8fd2bc95013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6d065-6b33-41f5-b245-d4a97b34770a",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc00355-4dc6-4c0d-a3d1-ff39b5e8a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_length(data, length):\n",
    "    return data[:length]\n",
    "    \n",
    "def load_data():\n",
    "    data = torch.load(\"data.pt\")\n",
    "    data = data.to(torch.float32)\n",
    "    train_data = train_data_length(data,TRAINING_DATA_LENGTH)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ce02bc-0c1f-4c4b-ac37-1acfc41c9ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_LENGTH = 1024\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = load_data()\n",
    "train_labels = torch.zeros(TRAINING_DATA_LENGTH)\n",
    "train_set = [(train_data[i], train_labels[i]) for i in range(TRAINING_DATA_LENGTH)]\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc93ec2-f3be-4c41-a43e-d2463cbe0073",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa85e441-c3dd-4ced-89c5-23e6d41f11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Before training the models, need to define some parameters\n",
    "RANDOM_SEED = 77  # Ensures reproducibility in the randomness\n",
    "LEARNING_RATE = 0.001  # Used to adapt the network weights\n",
    "NUM_EPOCHS = 10  # How many repetitions of training using the whole training set will be performed\n",
    "loss_function = nn.BCELoss()\n",
    "optimiser_discriminator = optim.Adam(discriminator.parameters(), lr = LEARNING_RATE)\n",
    "optimiser_generator = optim.Adam(generator.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb985e36-ca7a-4d72-bb4f-6b14790c09e9",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f79842d-2a05-4794-9cc9-594354c1d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b6fce0ce264decb3b6479d03b6df2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss D.: 50.00044631958008\n",
      "Epoch: 0 Loss G.: 8.099625587463379\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9ca7e263214bdb80acfefae8e231c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6a1ef4fd1448f5ad6262c6539688c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ed65cec53343cc84b153f8cdbb1836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d835dd85e7d49d980a138ec0fb6050d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ccc844524f44c8b591aff944c88b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4ca1926766405d831c5fe7a45cbd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52590c97fa194596bf7455c7d41b8f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e57a6b0ebb444e9a0d9973e4e2fded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78619673dfbe461abe8c112cb854c93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time for 10 epoch(s) is 4.73 seconds.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # Taking the real samples of the current batch from the data loader and assign them to real_samples\n",
    "    # The first dimension of the tensor has the number of elements equal to the batch size. \n",
    "    # This is the standard way of organising data in PyTorch, with each line of the tensor representing one sample from the batch.\n",
    "    for n, (real_samples, _) in enumerate(tqdm(train_loader)):\n",
    "        # DATA FOR TRAINING THE DISCRIMINATOR\n",
    "\n",
    "        # Using torch.ones() to create labels with the value 1 for real samples, and then assign to real_samples_labels\n",
    "        real_samples_labels = torch.ones((BATCH_SIZE, 1))\n",
    "\n",
    "        # Create the generated samples by storing random data in latent_space_samples\n",
    "        # This is fed into the generator to obtain generated_samples\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "        latent_space_samples = torch.randn((BATCH_SIZE, 2))\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "\n",
    "\n",
    "        # Use torch.zeros() to assign 0 to the labels for the generated samples\n",
    "        generated_samples_labels = torch.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "\n",
    "        # Concatenate the real and generated samples and labels and store them in all_samples\n",
    "        # and all_samples_labels to train the discriminator\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "\n",
    "\n",
    "        # TRAINING THE DISCRIMINATOR\n",
    "\n",
    "        # Clear the gradients at each training step to avoid accumulating them\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Calculate the output of the discriminator from the training data in all_samples\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "\n",
    "        # Calculate the loss function using discriminator output and all the labels\n",
    "        loss_discriminator = loss_function(output_discriminator, all_samples_labels)\n",
    "        \n",
    "        # Calculate the gradients to update the weights\n",
    "        loss_discriminator.backward()\n",
    "\n",
    "        # Update the discriminator weights\n",
    "        optimiser_discriminator.step()\n",
    "\n",
    "\n",
    "\n",
    "        # DATA FOR TRAINING THE GENERATOR\n",
    "        \n",
    "        # Storing random data in latent_space_samples with a number of lines to equal batch_size\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "        latent_space_samples = torch.randn((BATCH_SIZE, 2))\n",
    "\n",
    "        # TRAINING THE GENERATOR\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "\n",
    "        # Feeding generator's output into the discriminator and store its output, which is used\n",
    "        # as the output of the whole model\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "\n",
    "        # Calculate the loss function\n",
    "        loss_generator = loss_function(output_discriminator_generated, real_samples_labels)\n",
    "        \n",
    "        # Calculate and update the gradients\n",
    "        # REMEMBER:\n",
    "        # When the generator is trained, the discriminator weights are frozen since optimiser_generator\n",
    "        # was created with its first argument equal to generator.parameters()\n",
    "        loss_generator.backward()\n",
    "        optimiser_generator.step()\n",
    "\n",
    "\n",
    "        # Show loss\n",
    "        if epoch % 10 == 0 and n == BATCH_SIZE - 1:\n",
    "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = round(end_time - start_time, 2)\n",
    "print(f\"Run time for {NUM_EPOCHS} epoch(s) is {run_time} seconds.\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235a5db-4dcf-4d0b-a145-bfb2b37701aa",
   "metadata": {},
   "source": [
    "## Generating synthetic data from the trained adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c10373-2e33-475e-9dde-5f1b6924df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Duration      Size\n",
      "0   -0.099430 -0.031040\n",
      "1   -0.116570  0.035047\n",
      "2   -0.287762  0.034298\n",
      "3   -0.164122  0.091254\n",
      "4   -0.078110 -0.051378\n",
      "..        ...       ...\n",
      "995 -0.095357 -0.005502\n",
      "996 -0.110841 -0.021174\n",
      "997 -0.089764 -0.039157\n",
      "998 -0.209945  0.062785\n",
      "999 -0.249876 -0.099508\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Using 1000 samples of random noise to generate 1000 samples of synthetic data\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "latent_space_samples = torch.randn(1000, 2)\n",
    "generated_samples = generator(latent_space_samples)\n",
    "\n",
    "generated_samples = generated_samples.detach().numpy()\n",
    "df = pd.DataFrame(generated_samples, columns = [\"Duration\", \"Size\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa56d8-2e74-4375-aaeb-c6c1ebf8c605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
