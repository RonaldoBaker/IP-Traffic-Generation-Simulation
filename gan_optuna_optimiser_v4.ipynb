{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71dfc09-55e2-4d52-9d95-fd3cc90c8cd8",
   "metadata": {},
   "source": [
    "# Optimising the GAN to produce flow duration and flow size using Optuna\n",
    "\n",
    "Following three examples:\n",
    "\n",
    "1. https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837\n",
    "This example is a simple example to follow.\n",
    "\n",
    "2. https://github.com/optuna/optuna-examples/blob/main/multi_objective/pytorch_simple.py\n",
    "This example is more complicated but demonstrates how to have multi-objective optimisation, which is key in optimising the GAN, where we need to minimise loss for both neural networks.\n",
    "\n",
    "3. https://gitlab.com/hpo-uq/applications/gan4hep/-/blob/main/gan4hep/train_gan_2angles.py?ref_type=heads\n",
    "This example comes from the HYPPO paper, 'gan4hep' module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0c4355-74fa-4577-9cd9-0f198d3df65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613e32a-6ca2-45cf-98ac-4dbb0748753f",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0757c85-8d32-492d-a6bf-fdb3d16c68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_LENGTH = 3030487\n",
    "TRAINING_DATA = \"training_data_tensor.pt\"\n",
    "MIN_TRIALS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ec6f6-5ef3-4067-b87d-66b0276d0d32",
   "metadata": {},
   "source": [
    "## Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04d43b5-e2d7-46d5-8049-7ca51f189848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is 2D, first hidden layer is composed of 256 neurons with ReLU activation\n",
    "            nn.Linear(2, 128), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Have to use dropout to avoid overfitting\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # second and third layers are composed to 128 and 64 neurons, respectively\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # output is composed of a single neuron with sigmoidal activation to represent a probability\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee49249-76f2-4f9b-8803-f6d4c3630004",
   "metadata": {},
   "source": [
    "## Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7990e8e5-a832-4ee7-bbaf-0f45a27adb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506696ff-9aba-42af-936e-3ad1f4c21d95",
   "metadata": {},
   "source": [
    "## Pruner function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c5a86f-f7ac-4190-b628-d78bf1562ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trial(study, current_g_loss, current_d_loss):\n",
    "    \n",
    "    should_prune = False\n",
    "    \n",
    "    if current_g_loss == 0.0 or current_d_loss == 0.0: # one of the networks are no longer learning\n",
    "        should_prune = True\n",
    "    \n",
    "    else: \n",
    "        historic_g_loss = []\n",
    "        historic_d_loss = []\n",
    "    \n",
    "        for trial in study.trials:\n",
    "            # Only considering/calculating trials that have been completed\n",
    "            if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "                historic_g_loss.append(trial.values[0]) # accessing and appending the historic value for generator loss\n",
    "                historic_d_loss.append(trial.values[1]) # doing the same for the discriminator loss\n",
    "        \n",
    "                # Convert to numpy arrays\n",
    "                array_g_loss = np.array(historic_g_loss)\n",
    "                array_d_loss = np.array(historic_d_loss)\n",
    "        \n",
    "                # Calculate the mean losses\n",
    "                mean_g_loss = np.mean(array_g_loss)\n",
    "                mean_d_loss = np.mean(array_d_loss)\n",
    "\n",
    "        should_prune = current_g_loss > mean_g_loss or current_d_loss > mean_d_loss\n",
    "        \n",
    "    return should_prune\n",
    "\n",
    "    \n",
    "def prune(study, min_trials, current_g_loss, current_d_loss):\n",
    "    should_prune = False\n",
    "    completed_trials = 0\n",
    "    \n",
    "    # checking how many trials are complete to compare to the current trial\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            completed_trials += 1\n",
    "    \n",
    "    # if there are sufficient trials to compare, it will compare\n",
    "    if completed_trials >= min_trials:\n",
    "        should_prune = evaluate_trial(study, current_g_loss, current_d_loss)\n",
    "        if should_prune:\n",
    "            print(\"Trial has been pruned\")\n",
    "            raise optuna.TrialPruned()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92660876-6c93-402c-a7a2-c0ace1f03703",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ce02bc-0c1f-4c4b-ac37-1acfc41c9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = torch.load(TRAINING_DATA)\n",
    "    data = data.to(torch.float32)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89e0403-1680-4896-bc3b-2a6c201b2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_optimise(params, generator, discriminator, study):\n",
    "\n",
    "    # loading and moving the training data and models to the GPU if it is available\n",
    "    train_data = load_data()\n",
    "    train_data = train_data.to(DEVICE)\n",
    "    train_labels = torch.zeros(size=(TRAINING_DATA_LENGTH, 1))\n",
    "    train_labels = train_labels.to(DEVICE)\n",
    "    train_set = [(train_data[i], train_labels[i]) for i in range(TRAINING_DATA_LENGTH)]\n",
    "    train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"], shuffle=True, drop_last = True)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    discriminator_optimiser = optim.Adam(discriminator.parameters(), lr = params[\"learning_rate\"])\n",
    "    generator_optimiser = optim.Adam(generator.parameters(), lr = params[\"learning_rate\"])\n",
    "\n",
    "    \n",
    "    discriminator = discriminator.to(DEVICE)\n",
    "    generator = generator.to(DEVICE)\n",
    "    loss_function = loss_function.to(DEVICE)\n",
    "    \n",
    "    warmup_epochs = params[\"epochs\"] // 2\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        \n",
    "        for n, (real_samples, _) in enumerate(tqdm(train_loader)):\n",
    "            \n",
    "            # DATA FOR DISCRIMINATOR\n",
    "            torch.manual_seed(RANDOM_SEED)\n",
    "            real_samples_labels = torch.ones((params[\"batch_size\"], 1), device = DEVICE)\n",
    "            \n",
    "            latent_space_samples = torch.randn((params[\"batch_size\"], 2), device = DEVICE)\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            generated_samples_labels = torch.zeros((params[\"batch_size\"], 1), device = DEVICE)\n",
    "            \n",
    "            all_samples = torch.cat((real_samples, generated_samples))\n",
    "            all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            discriminator.zero_grad()\n",
    "            output_discriminator = discriminator(all_samples)\n",
    "            discriminator_loss = loss_function(output_discriminator, all_samples_labels)\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimiser.step()\n",
    "\n",
    "\n",
    "            # DATA FOR GENERATOR\n",
    "            torch.manual_seed(RANDOM_SEED)\n",
    "            latent_space_samples = torch.randn((params[\"batch_size\"], 2), device = DEVICE)\n",
    "\n",
    "            # TRAINING GENERATOR\n",
    "            generator.zero_grad()\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            output_discriminator_generated = discriminator(generated_samples)\n",
    "            generator_loss = loss_function(output_discriminator_generated, real_samples_labels)\n",
    "            generator_loss.backward()\n",
    "            generator_optimiser.step()\n",
    "\n",
    "            if epoch % 1 == 0 and n == params[\"batch_size\"] - 1:\n",
    "                print(f\"Epoch: {epoch} | G. Loss: {generator_loss} | D. Loss: {discriminator_loss}\")\n",
    "\n",
    "        # Pruning\n",
    "        if epoch == warmup_epochs: \n",
    "            # The trial is half way through completion, now checking whether to prune or not\n",
    "            prune(study = study, min_trials = MIN_TRIALS, min_epochs = warmup_epochs, current_epoch = epoch, \n",
    "                current_g_loss = generator_loss, current_d_loss = discriminator_loss)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    run_time = round(end_time - start_time, 2)\n",
    "    print(f\"Trial Complete!\\nRun time for this trial was {run_time} seconds.\\n\")\n",
    "\n",
    "    return generator_loss, discriminator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235a5db-4dcf-4d0b-a145-bfb2b37701aa",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "The aim of this function is to define a set of hyperparameter values, build the model, train the model, and evaluate the loss of both the generator and discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4efa56d8-2e74-4375-aaeb-c6c1ebf8c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, additional_arg):\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", low=1e-5, high=1e-1, log = True),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", low=500, high=2000, step=100),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", low=5, high=50, step=5),\n",
    "        \n",
    "    }\n",
    "\n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    g_loss, d_loss = train_and_optimise(params, generator, discriminator, additional_arg)\n",
    "\n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b441bf4-f807-4d7f-8186-3169bd427e8c",
   "metadata": {},
   "source": [
    "## Display all trials and best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a71587-7270-4e35-9e7a-abcf7dd61751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_trials(study):\n",
    "    df = study.trials_dataframe()\n",
    "    df = pd.DataFrame(df, columns = ['number', 'values_0', 'values_1', 'params_batch_size', 'params_epochs',\n",
    "                                          'params_learning_rate', 'state'])\n",
    "    df= df.rename(columns = {\"number\":\"Trial #\", \"values_0\":\"G Loss\", \"values_1\":\"D Loss\", \"params_batch_size\":\"Batch Size\", \n",
    "                     \"params_epochs\":\"Epochs\", \"params_learning_rate\":\"Learning Rate\", \"state\":\"State\"})\n",
    "    df[\"Trial #\"] += 1 # Adjust the trial numbers\n",
    "    \n",
    "    total = len(df)\n",
    "    complete = 0\n",
    "    pruned = 0\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            complete += 1    \n",
    "        elif trial.state == optuna.trial.TrialState.PRUNED:\n",
    "            pruned += 1\n",
    "    \n",
    "    print(f\"{total} TOTAL TRIALS \\n{complete} COMPLETED TRIALS \\n{pruned} PRUNED TRIALS\\n\")\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "def display_best_trial(study):\n",
    "    best_trial = study.best_trials\n",
    "    best_trial_number = best_trial[0].number\n",
    "    best_trial_params = best_trial[0].params\n",
    "    best_trial_values = best_trial[0].values\n",
    "    print(\"~Best trial~\")\n",
    "    print(f\"Trial #: {best_trial_number+1}\")\n",
    "    print(f\"G Loss: {best_trial_values[0]}\\nD Loss: {best_trial_values[1]}\")\n",
    "    print(f\"Batch Size: {best_trial_params['batch_size']}\\nEpochs: {best_trial_params['epochs']}\\nLearning Rate: {best_trial_params['learning_rate']}\")\n",
    "\n",
    "def save_trials(study):\n",
    "    df = study.trials_dataframe()\n",
    "    now = str(datetime.now())\n",
    "    date, time = now.split(\" \")\n",
    "    time = time.replace(\":\",\".\")\n",
    "    df.to_csv(f\"Optimisation Trials/gan_optimisation_{date}_{time}.csv\", index = False)\n",
    "    print(f\"Trials saved to: gan_optimisation_{date}_{time}.csv\\n\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0c69c1-f88c-44b4-82be-a259ae4495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(seconds):\n",
    "    seconds = seconds % (24 * 3600)\n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "     \n",
    "    return \"%d hours %02d mins %02d secs\" % (hour, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbae1bc-d829-465e-92ef-0c3f0bf2d421",
   "metadata": {},
   "source": [
    "## Main program\n",
    "The study that is created provides a multi-objective optimisation, so that it can optimise more than one value - generator loss and discriminator loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85c9b09-dc94-47a0-b1c3-c93af0d2c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-08 13:36:18,594] A new study created in RDB with name: GAN-Optimiser\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new optimisation loop...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d29bd0cfce4b84ad7f7a63aca28b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-04-08 13:37:32,423] Trial 0 failed with parameters: {'learning_rate': 0.00046574321034245544, 'batch_size': 2000, 'epochs': 30} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\rrema\\AppData\\Local\\Temp\\ipykernel_7688\\2797391794.py\", line 12, in objective\n",
      "    g_loss, d_loss = train_and_optimise(params, generator, discriminator, additional_arg)\n",
      "  File \"C:\\Users\\rrema\\AppData\\Local\\Temp\\ipykernel_7688\\1547163394.py\", line 41, in train_and_optimise\n",
      "    output_discriminator = discriminator(all_samples)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\rrema\\AppData\\Local\\Temp\\ipykernel_7688\\3483308868.py\", line 27, in forward\n",
      "    output = self.model(x)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\rrema\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2024-04-08 13:37:32,432] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting new optimisation loop...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Number of trials to test with different values\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     38\u001b[0m optimisation_run_time \u001b[38;5;241m=\u001b[39m convert_time(\u001b[38;5;28mround\u001b[39m(end_time \u001b[38;5;241m-\u001b[39m start_time, \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, additional_arg)\u001b[0m\n\u001b[0;32m     10\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[0;32m     11\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator()\n\u001b[1;32m---> 12\u001b[0m g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_optimise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_loss, d_loss\n",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mtrain_and_optimise\u001b[1;34m(params, generator, discriminator, study)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# TRAINING DISCRIMINATOR\u001b[39;00m\n\u001b[0;32m     40\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 41\u001b[0m output_discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m discriminator_loss \u001b[38;5;241m=\u001b[39m loss_function(output_discriminator, all_samples_labels)\n\u001b[0;32m     43\u001b[0m discriminator_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 27\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    interrupted = False\n",
    "    \n",
    "    # Create the optimisation study\n",
    "    study = optuna.create_study(directions=[\"minimize\", \"maximize\"], \n",
    "                                study_name = \"GAN-Optimiser\",\n",
    "                                sampler = optuna.samplers.NSGAIISampler(), \n",
    "                                storage = \"sqlite:///demonstration_optimisation.db\", \n",
    "                                load_if_exists=True)\n",
    "\n",
    "    additional_arg = study\n",
    "    wrapped_objective = functools.partial(objective, additional_arg = additional_arg)\n",
    "    \n",
    "    # checking if an optimisation loop was halted and there are incomplete trials\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.FAIL or trial.state == optuna.trial.TrialState.WAITING:\n",
    "            interrupted = True\n",
    "            study.enqueue_trial(trial.params)\n",
    "\n",
    "\n",
    "    # Optimise the objective function \n",
    "            \n",
    "    if interrupted: # carrying on from previously started optimisation loop\n",
    "        complete_trials = 0\n",
    "        for trial in study.trials:\n",
    "            if trial.state == optuna.trial.TrialState.PRUNED or trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "                complete_trials +=1 \n",
    "        print(\"Continuing previous optimisation loop...\")\n",
    "        study.optimize(wrapped_objective, n_trials = 200 - complete_trials)\n",
    "\n",
    "    else: # starting a completely new study\n",
    "        print(\"Starting new optimisation loop...\")\n",
    "        start_time = time.time()\n",
    "        study.optimize(wrapped_objective, n_trials = 150) # Number of trials to test with different values\n",
    "        end_time = time.time()\n",
    "\n",
    "    optimisation_run_time = convert_time(round(end_time - start_time, 2))\n",
    "    \n",
    "    print(f\"Optimsation complete!\\nOptimisation run time was {optimisation_run_time}\\n\")\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    save_trials(study)\n",
    "    display_all_trials(study)\n",
    "    display_best_trial(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a686bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
